#!/usr/bin/env python3
import os
import json
import logging
import requests
import sys
from typing import List, Dict, Any
import pytest
from pytest_bdd import given, when, then, parsers
import openai
from dotenv import load_dotenv
from datetime import datetime
from pathlib import Path
from requests_html import HTML, HTMLSession
import itertools

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables from .env file
load_dotenv()

# Configure OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")

def scrape_swagger_ui(url: str) -> List[str]:
    """
    Scrapes a Swagger UI page and extracts individual endpoint operations.
    
    Args:
        url: The URL of the Swagger UI page
        
    Returns:
        List of HTML chunks, each representing a single operation (method + path)
    """
    session = HTMLSession()
    response = session.get(url)
    
    if response.status_code != 200:
        raise Exception(f"Failed to fetch Swagger UI page. Status code: {response.status_code}")
    
    # Render the page if JavaScript is needed
    try:
        response.html.render(timeout=30)
    except Exception as e:
        logger.warning(f"JavaScript rendering failed: {str(e)}")
        logger.info("Continuing with static HTML content...")
    
    # Extract all operation blocks using specific opblock classes
    opblock_classes = ['opblock-get', 'opblock-post', 'opblock-put', 'opblock-delete', 'opblock-patch']
    operation_chunks = []
    
    for op_class in opblock_classes:
        blocks = response.html.find(f'div.{op_class}')
        for block in blocks:
            operation_chunks.append(block.html)
    
    if not operation_chunks:
        logger.warning("No operation blocks found. The page might not be a Swagger UI or the selectors might need adjustment.")
    
    return operation_chunks

def extract_endpoint_info_via_llm(chunk_html: str, model_name="gpt-3.5-turbo") -> Dict[str, Any]:
    """
    Extracts structured endpoint information from a Swagger UI HTML chunk using an LLM.
    
    Args:
        chunk_html: HTML chunk containing a single endpoint operation
        model_name: Name of the LLM model to use
        
    Returns:
        Dict containing structured endpoint information with keys:
        {
            "endpoint": str,  # e.g. "/books"
            "method": str,    # e.g. "GET"
            "parameters": List[Dict],  # List of parameter objects
            "requestBody": Dict,  # Request body schema if applicable
            "responses": Dict  # Response schemas by status code
        }
        Returns empty dict if no valid data is found.
    """
    logger.debug("Processing HTML chunk with LLM")

    openai.api_key = os.getenv("OPENAI_API_KEY", "")
    if not openai.api_key:
        raise RuntimeError("Missing OPENAI_API_KEY environment variable.")

    prompt_text = f"""
Extract structured API endpoint information from this Swagger UI HTML snippet.
Return a single JSON object (not an array) with the following structure:

{{
    "endpoint": "string (e.g. /books)",
    "method": "string (GET/POST/PUT/DELETE/PATCH)",
    "parameters": [
        {{
            "name": "string",
            "in": "string (path/query/header/cookie)",
            "required": boolean,
            "type": "string (e.g. string/number/integer/boolean)",
            "description": "string",
            "schema": {{}}  # Optional schema information
        }}
    ],
    "requestBody": {{
        "required": boolean,
        "content": {{
            "application/json": {{
                "schema": {{}}  # Request body schema
            }}
        }}
    }},
    "responses": {{
        "200": {{  # Status code as key
            "description": "string",
            "content": {{
                "application/json": {{
                    "schema": {{}}  # Response schema
                }}
            }}
        }}
    }}
}}

Important:
1. Handle all parameter types (path, query, header, cookie)
2. Include request body schema if present
3. Include response schemas for all status codes
4. Return a single object, not an array
5. Return empty object {{}} if no valid data found

HTML:
{chunk_html}
"""

    try:
        response = openai.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "You are an API documentation analyzer. Extract structured endpoint information from Swagger UI HTML and return only valid JSON without any additional text."},
                {"role": "user", "content": prompt_text}
            ],
            max_tokens=2000,
            temperature=0.0,
        )
        
        completion_text = response.choices[0].message.content.strip()
        completion_text = completion_text.strip().replace('```json', '').replace('```', '')
        
        data = json.loads(completion_text)
        
        # Validate required fields
        required_fields = ["endpoint", "method"]
        if not all(field in data for field in required_fields):
            logger.warning("Missing required fields in LLM response")
            return {}
            
        # Ensure parameters is a list
        if "parameters" not in data:
            data["parameters"] = []
        elif not isinstance(data["parameters"], list):
            data["parameters"] = []
            
        # Ensure requestBody is a dict
        if "requestBody" not in data:
            data["requestBody"] = {}
        elif not isinstance(data["requestBody"], dict):
            data["requestBody"] = {}
            
        # Ensure responses is a dict
        if "responses" not in data:
            data["responses"] = {}
        elif not isinstance(data["responses"], dict):
            data["responses"] = {}
            
        return data
        
    except json.JSONDecodeError as e:
        logger.error(f"Invalid JSON from LLM: {str(e)}")
        return {}
    except Exception as e:
        logger.error(f"Error processing HTML chunk: {str(e)}")
        return {}

def generate_step_definitions(feature_file: str, endpoint_info: Dict[str, Any], output_dir: str = "steps") -> str:
    """
    Generates pytest-bdd step definitions for a feature file.
    
    Args:
        feature_file: Path to the .feature file
        endpoint_info: Dictionary containing endpoint information
        output_dir: Directory to write step definition files
        
    Returns:
        Path to the generated step definitions file
    """
    logger.info(f"Generating step definitions for {feature_file}")
    os.makedirs(output_dir, exist_ok=True)
    
    # Generate step file name from feature file name
    step_file = os.path.join(output_dir, f"{os.path.splitext(os.path.basename(feature_file))[0]}_steps.py")
    
    # Extract endpoint details
    path = endpoint_info["endpoint"]
    method = endpoint_info["method"]
    parameters = endpoint_info.get("parameters", [])
    request_body = endpoint_info.get("requestBody", {})
    responses = endpoint_info.get("responses", {})
    
    # Generate step definitions content
    step_content = f'''#!/usr/bin/env python3
import os
import json
import pytest
from pytest_bdd import given, when, then, parsers
import requests
from typing import Dict, Any
from datetime import datetime

# API Configuration
BASE_URL = os.getenv("API_BASE_URL", "https://api.example.com")
AUTH_TOKEN = os.getenv("API_AUTH_TOKEN", "")

# Request context
request_context = {{}}

@given("the API service is running")
def api_service_running():
    """Verify the API service is accessible."""
    response = requests.get(f"{{BASE_URL}}/health")
    assert response.status_code == 200

@given("I have valid authentication credentials")
def valid_credentials():
    """Set up authentication credentials."""
    assert AUTH_TOKEN, "API_AUTH_TOKEN environment variable is required"

@given(parsers.parse("I am making a {method} request to \"{path}\""))
def prepare_request(method: str, path: str):
    """Prepare the request context."""
    request_context["method"] = method
    request_context["path"] = path
    request_context["headers"] = {{
        "Authorization": f"Bearer {{AUTH_TOKEN}}",
        "Content-Type": "application/json"
    }}
    request_context["params"] = {{}}
    request_context["data"] = None

@given("I include the following optional parameters")
def add_optional_parameters(parameters_table):
    """Add optional parameters to the request."""
    for row in parameters_table:
        param_name = row["parameter"]
        param_value = row["value"]
        request_context["params"][param_name] = param_value

@given("I have the following request body")
def set_request_body(request_body_table):
    """Set the request body from the table."""
    body = {{}}
    for row in request_body_table:
        field = row["field"]
        value = row["value"]
        body[field] = value
    request_context["data"] = json.dumps(body)

@when("I send the request")
def send_request():
    """Send the prepared request."""
    url = f"{{BASE_URL}}{{request_context['path']}}"
    response = requests.request(
        method=request_context["method"],
        url=url,
        headers=request_context["headers"],
        params=request_context["params"],
        data=request_context["data"]
    )
    request_context["response"] = response

@when("I send the request without required parameters")
def send_request_without_required():
    """Send request without required parameters."""
    url = f"{{BASE_URL}}{{request_context['path']}}"
    response = requests.request(
        method=request_context["method"],
        url=url,
        headers=request_context["headers"]
    )
    request_context["response"] = response

@when("I send the request with invalid data")
def send_request_with_invalid_data():
    """Send request with invalid data."""
    url = f"{{BASE_URL}}{{request_context['path']}}"
    invalid_data = {{"invalid": "data"}}
    response = requests.request(
        method=request_context["method"],
        url=url,
        headers=request_context["headers"],
        data=json.dumps(invalid_data)
    )
    request_context["response"] = response

@when("I send the request without authentication")
def send_request_without_auth():
    """Send request without authentication."""
    url = f"{{BASE_URL}}{{request_context['path']}}"
    headers = {{"Content-Type": "application/json"}}
    response = requests.request(
        method=request_context["method"],
        url=url,
        headers=headers
    )
    request_context["response"] = response

@when("I send the request for a non-existent resource")
def send_request_for_nonexistent():
    """Send request for a non-existent resource."""
    url = f"{{BASE_URL}}{{request_context['path']}}/nonexistent"
    response = requests.request(
        method=request_context["method"],
        url=url,
        headers=request_context["headers"]
    )
    request_context["response"] = response

@then(parsers.parse("the response status code should be {status_code:d}"))
def verify_status_code(status_code: int):
    """Verify the response status code."""
    assert request_context["response"].status_code == status_code

@then("the response should contain valid data")
def verify_valid_response():
    """Verify the response contains valid data."""
    response = request_context["response"]
    assert response.status_code == 200
    data = response.json()
    assert data is not None
    # Add specific data validation based on response schema

@then("the response should match the expected schema")
def verify_response_schema():
    """Verify the response matches the expected schema."""
    response = request_context["response"]
    data = response.json()
    # Add schema validation logic here
    # This could use a library like jsonschema

@then("the response should indicate missing required parameters")
def verify_missing_params():
    """Verify the response indicates missing required parameters."""
    response = request_context["response"]
    data = response.json()
    assert "error" in data
    assert "missing" in data["error"].lower()

@then("the response should contain error details")
def verify_error_details():
    """Verify the response contains error details."""
    response = request_context["response"]
    data = response.json()
    assert "error" in data
    assert "details" in data

@then("the response should indicate authentication required")
def verify_auth_required():
    """Verify the response indicates authentication is required."""
    response = request_context["response"]
    data = response.json()
    assert "error" in data
    assert "unauthorized" in data["error"].lower()

@then("the response should indicate resource not found")
def verify_not_found():
    """Verify the response indicates resource not found."""
    response = request_context["response"]
    data = response.json()
    assert "error" in data
    assert "not found" in data["error"].lower()
'''
    
    # Write the step definitions file
    with open(step_file, 'w') as f:
        f.write(step_content)
    
    logger.info(f"Generated step definitions file: {step_file}")
    return step_file

def validate_scenario_parameters(scenario: str, endpoint_info: Dict[str, Any]) -> bool:
    """
    Validates that all parameters referenced in a scenario exist in the endpoint info.
    
    Args:
        scenario: The scenario text to validate
        endpoint_info: The endpoint information containing valid parameters
        
    Returns:
        bool: True if all parameters are valid, False otherwise
    """
    # Extract all parameter names from the scenario
    param_references = set()
    for line in scenario.split('\n'):
        if '|' in line:
            # Handle table parameters
            params = [p.strip() for p in line.split('|')[1:-1]]
            param_references.update(params)
        elif '<' in line and '>' in line:
            # Handle scenario outline parameters
            params = [p.strip() for p in line.split('<')[1].split('>')[0].split(',')]
            param_references.update(params)
    
    # Get valid parameter names from endpoint info
    valid_params = {p['name'] for p in endpoint_info.get('parameters', [])}
    if 'requestBody' in endpoint_info:
        body_schema = endpoint_info['requestBody'].get('content', {}).get('application/json', {}).get('schema', {})
        if 'properties' in body_schema:
            valid_params.update(body_schema['properties'].keys())
    
    # Check for invalid parameters
    invalid_params = param_references - valid_params
    if invalid_params:
        logger.warning(f"Found invalid parameter references: {invalid_params}")
        return False
    
    return True

def generate_parameter_combinations(parameters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Generates parameter combinations for scenario outlines.
    
    Args:
        parameters: List of parameter definitions
        
    Returns:
        List of parameter combinations
    """
    required_params = [p for p in parameters if p.get('required', False)]
    optional_params = [p for p in parameters if not p.get('required', False)]
    
    combinations = []
    
    # Generate combinations with required parameters
    base_combo = {p['name']: f"valid_{p['name']}" for p in required_params}
    combinations.append(base_combo)
    
    # Add combinations with optional parameters
    for i in range(1, len(optional_params) + 1):
        for combo in itertools.combinations(optional_params, i):
            combo_dict = base_combo.copy()
            for param in combo:
                combo_dict[param['name']] = f"valid_{param['name']}"
            combinations.append(combo_dict)
    
    return combinations

def generate_feature_files(endpoints: List[Dict[str, Any]], output_dir: str = "features", model_name: str = "gpt-3.5-turbo") -> List[str]:
    """
    Generates comprehensive Gherkin feature files for each endpoint.
    
    Args:
        endpoints: List of endpoint information from assemble_endpoints_from_chunks
        output_dir: Directory to write feature files
        model_name: Name of the LLM model to use
        
    Returns:
        List of paths to generated feature files
    """
    logger.info(f"Generating feature files for {len(endpoints)} endpoints")
    os.makedirs(output_dir, exist_ok=True)
    feature_files = []
    step_files = []
    
    for endpoint in endpoints:
        path = endpoint["endpoint"]
        method = endpoint["method"]
        parameters = endpoint.get("parameters", [])
        request_body = endpoint.get("requestBody", {})
        responses = endpoint.get("responses", {})
        
        # Generate feature file name
        feature_name = f"{method}_{path.strip('/').replace('/', '_')}.feature"
        feature_path = os.path.join(output_dir, feature_name)
        
        # Prepare parameter scenarios
        param_scenarios = []
        required_params = [p for p in parameters if p.get("required", False)]
        optional_params = [p for p in parameters if not p.get("required", False)]
        
        # Generate parameter combinations for scenario outlines
        param_combinations = generate_parameter_combinations(parameters)
        
        # Generate parameter test scenarios
        if required_params:
            param_scenarios.append(f"""
  @negative @validation
  Scenario: Missing required parameters
    Given I am making a {method} request to "{path}"
    When I send the request without required parameters
    Then the response status code should be 400
    And the response should indicate missing required parameters
""")
        
        if param_combinations:
            param_scenarios.append(f"""
  @validation
  Scenario Outline: Parameter combinations
    Given I am making a {method} request to "{path}"
    And I include the following parameters:
      | parameter | value |
      {chr(10).join(f"      | {name} | {value} |" for name, value in combo.items())}
    When I send the request
    Then the response status code should be <status>
    And the response should contain <validation>

  Examples:
    | status | validation |
    | 200 | "successful response" |
    | 400 | "invalid parameters" |
""")
        
        # Generate request body scenarios if applicable
        body_scenarios = []
        if request_body and method in ["POST", "PUT", "PATCH"]:
            body_schema = request_body.get("content", {}).get("application/json", {}).get("schema", {})
            if body_schema:
                # Valid payload scenario
                body_scenarios.append(f"""
  @validation
  Scenario: Valid request body
    Given I am making a {method} request to "{path}"
    And I have the following valid request body:
      | field | value |
      {chr(10).join(f"      | {field} | {field} |" for field in body_schema.get('properties', {}).keys())}
    When I send the request
    Then the response status code should be 200
    And the response should contain valid data
""")
                
                # Invalid payload scenarios
                body_scenarios.append(f"""
  @negative @validation
  Scenario Outline: Invalid request body validation
    Given I am making a {method} request to "{path}"
    And I have the following invalid request body:
      | field | value |
      | <field> | <value> |
    When I send the request
    Then the response status code should be 400
    And the response should contain <validation>

  Examples:
    | field | value | validation |
    | title | "" | "Title cannot be empty" |
    | price | -1 | "Price must be positive" |
    | isbn | "invalid" | "Invalid ISBN format" |
""")
        
        # Generate response validation scenarios
        response_scenarios = []
        for status_code, response in responses.items():
            if status_code == "200":
                response_scenarios.append(f"""
  @validation
  Scenario: Successful response validation
    Given I am making a {method} request to "{path}"
    When I send the request with valid data
    Then the response status code should be 200
    And the response should contain valid data
    And the response should match the expected schema
""")
            elif status_code == "400":
                response_scenarios.append(f"""
  @negative @validation
  Scenario: Bad request validation
    Given I am making a {method} request to "{path}"
    When I send the request with invalid data
    Then the response status code should be 400
    And the response should contain error details
""")
            elif status_code == "401":
                response_scenarios.append(f"""
  @negative @auth
  Scenario: Unauthorized access
    Given I am making a {method} request to "{path}"
    When I send the request without authentication
    Then the response status code should be 401
    And the response should indicate authentication required
""")
            elif status_code == "404":
                response_scenarios.append(f"""
  @negative @validation
  Scenario: Resource not found
    Given I am making a {method} request to "{path}"
    When I send the request for a non-existent resource
    Then the response status code should be 404
    And the response should indicate resource not found
""")
        
        # Validate all scenarios before writing
        all_scenarios = param_scenarios + body_scenarios + response_scenarios
        valid_scenarios = []
        for scenario in all_scenarios:
            if validate_scenario_parameters(scenario, endpoint):
                valid_scenarios.append(scenario)
            else:
                logger.warning(f"Removing invalid scenario from {feature_name}")
        
        # Generate the complete feature file content
        feature_content = f"""Feature: {method} {path} API Endpoint

  As an API client
  I want to interact with the {path} endpoint
  So that I can {method.lower()} resources

  Background:
    Given the API service is running
    And I have valid authentication credentials

{chr(10).join(valid_scenarios)}
"""
        
        # Write the feature file
        with open(feature_path, 'w') as f:
            f.write(feature_content)
        
        logger.info(f"Generated feature file: {feature_path}")
        feature_files.append(feature_path)
        
        # Generate step definitions
        step_file = generate_step_definitions(feature_path, endpoint)
        step_files.append(step_file)
    
    return feature_files, step_files

def generate_crud_e2e_scenarios(endpoints: list) -> dict:
    """
    Generate CRUD end-to-end scenarios by analyzing endpoint patterns.
    Returns a dictionary of feature files and their CRUD scenarios.
    """
    print("[DEBUG] Starting CRUD scenario generation")
    
    # Group endpoints by their base path
    endpoint_groups = {}
    for endpoint in endpoints:
        base_path = endpoint['endpoint'].split('/')[1]  # e.g., 'Account' or 'BookStore'
        if base_path not in endpoint_groups:
            endpoint_groups[base_path] = []
        endpoint_groups[base_path].append(endpoint)
    
    print(f"[DEBUG] Found {len(endpoint_groups)} endpoint groups")
    crud_scenarios = {}
    
    # Generate CRUD scenarios for each group
    for group_name, group_endpoints in endpoint_groups.items():
        print(f"[DEBUG] Processing group: {group_name}")
        feature_name = f"test_{group_name.lower()}_crud_e2e.feature"
        
        # Analyze endpoints to find CRUD operations
        crud_ops = {
            'create': [e for e in group_endpoints if e['method'] == 'POST'],
            'read': [e for e in group_endpoints if e['method'] == 'GET'],
            'update': [e for e in group_endpoints if e['method'] in ['PUT', 'PATCH']],
            'delete': [e for e in group_endpoints if e['method'] == 'DELETE']
        }
        
        print(f"[DEBUG] Found CRUD operations for {group_name}: {[k for k, v in crud_ops.items() if v]}")
        
        # Generate the feature file content with dynamic setup
        content = f"""Feature: End-to-End CRUD Testing for {group_name} API

Background:
    Given the API service is running
    And I am authenticated with valid credentials
"""
        
        # Generate CRUD scenarios dynamically based on available operations
        for operation, endpoints in crud_ops.items():
            if endpoints:
                endpoint = endpoints[0]  # Use the first matching endpoint
                print(f"[DEBUG] Adding {operation} scenario for {endpoint['endpoint']}")
                
                # Generate scenario based on operation type
                if operation == 'create':
                    content += f"""
@e2e @crud @{operation}
Scenario: Create {group_name} Resource
    Given I prepare test data for {group_name.lower()} creation
    When I send a POST request to "{endpoint['endpoint']}" with valid data
    Then the response status code should be 200
    And I store the created {group_name.lower()} ID
"""
                elif operation == 'read':
                    content += f"""
@e2e @crud @{operation}
Scenario: Read {group_name} Resource
    Given I have a valid {group_name.lower()} ID
    When I send a GET request to "{endpoint['endpoint']}"
    Then the response status code should be 200
    And the response should contain valid {group_name.lower()} data
"""
                elif operation == 'update':
                    content += f"""
@e2e @crud @{operation}
Scenario: Update {group_name} Resource
    Given I have a valid {group_name.lower()} ID
    And I prepare updated data for {group_name.lower()}
    When I send a {endpoint['method']} request to "{endpoint['endpoint']}"
    Then the response status code should be 200
    And the {group_name.lower()} should be updated
"""
                elif operation == 'delete':
                    content += f"""
@e2e @crud @{operation}
Scenario: Delete {group_name} Resource
    Given I have a valid {group_name.lower()} ID
    When I send a DELETE request to "{endpoint['endpoint']}"
    Then the response status code should be 200
    And the {group_name.lower()} should be deleted
"""
        
        # Generate integration scenarios dynamically
        integration_scenarios = []
        for other_group in endpoint_groups:
            if other_group != group_name:
                print(f"[DEBUG] Checking integration with {other_group}")
                if any(e['method'] == 'POST' for e in endpoint_groups[other_group]):
                    integration_scenarios.append(other_group)
        
        if integration_scenarios:
            print(f"[DEBUG] Adding integration scenarios with: {integration_scenarios}")
            content += f"""
@e2e @integration
Scenario: {group_name} Integration with Other Resources
    Given I have valid credentials
    And I am authenticated
"""
            
            for other_group in integration_scenarios:
                content += f"""
    # {other_group} Integration
    When I create a new {other_group.lower()} resource
    Then I can associate it with {group_name.lower()}
    And I can verify the association
"""
            
            content += """
    # Cleanup
    When I remove all created resources
    Then all resources should be properly cleaned up
"""
        
        crud_scenarios[feature_name] = content
        print(f"[DEBUG] Generated feature file: {feature_name}")
    
    return crud_scenarios

def generate_all_feature_files(api_endpoints: list, model_name="gpt-3.5-turbo"):
    """
    For each endpoint (dict) in api_endpoints, generate a .feature file and its corresponding step definitions.
    Also generates CRUD end-to-end scenarios.
    """
    print("[DEBUG] Entering generate_all_feature_files")
    print("[DEBUG] Total endpoints to process:", len(api_endpoints))

    feature_files = []
    step_files = []
    
    # First, restore backed up feature files
    if os.path.exists('features_backup'):
        print("[DEBUG] Restoring feature files from backup")
        os.system('cp -r features_backup/* features/')
    
    # Generate CRUD end-to-end scenarios
    crud_scenarios = generate_crud_e2e_scenarios(api_endpoints)
    
    # Save CRUD scenarios
    os.makedirs('features/crud', exist_ok=True)
    for feature_name, content in crud_scenarios.items():
        feature_path = f"features/crud/{feature_name}"
        with open(feature_path, 'w') as f:
            f.write(content)
        feature_files.append(feature_path)
        
        # Generate step definitions for CRUD scenarios
        step_file = generate_step_definitions(feature_path, 'features/crud')
        step_files.append(step_file)
    
    return feature_files, step_files

def assemble_endpoints_from_chunks(endpoint_chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Assembles and validates endpoint information from multiple chunks.
    
    Args:
        endpoint_chunks: List of endpoint info dictionaries from extract_endpoint_info_via_llm
        
    Returns:
        List of consolidated endpoint information, with duplicates removed and conflicts resolved
    """
    # Use a dict to store endpoints by path, allowing multiple methods per path
    endpoints_by_path: Dict[str, Dict[str, Any]] = {}
    
    for chunk in endpoint_chunks:
        if not chunk or "endpoint" not in chunk or "method" not in chunk:
            logger.warning("Skipping invalid endpoint chunk")
            continue
            
        path = chunk["endpoint"]
        method = chunk["method"].upper()
        
        if path not in endpoints_by_path:
            # Initialize new endpoint entry
            endpoints_by_path[path] = {
                "endpoint": path,
                "methods": {},
                "parameters": [],
                "requestBody": {},
                "responses": {}
            }
        
        # Check for method conflicts
        if method in endpoints_by_path[path]["methods"]:
            logger.warning(f"Found duplicate method {method} for endpoint {path}")
            # Compare parameters and request body for conflicts
            existing = endpoints_by_path[path]["methods"][method]
            if existing.get("parameters") != chunk.get("parameters", []):
                logger.warning(f"Parameter mismatch for {method} {path}")
            if existing.get("requestBody") != chunk.get("requestBody", {}):
                logger.warning(f"Request body mismatch for {method} {path}")
            if existing.get("responses") != chunk.get("responses", {}):
                logger.warning(f"Response mismatch for {method} {path}")
        
        # Store method-specific information
        endpoints_by_path[path]["methods"][method] = {
            "parameters": chunk.get("parameters", []),
            "requestBody": chunk.get("requestBody", {}),
            "responses": chunk.get("responses", {})
        }
        
        # Merge parameters, removing duplicates
        existing_params = {p["name"]: p for p in endpoints_by_path[path]["parameters"]}
        for param in chunk.get("parameters", []):
            if param["name"] in existing_params:
                if existing_params[param["name"]] != param:
                    logger.warning(f"Parameter conflict for {param['name']} in {path}")
            else:
                endpoints_by_path[path]["parameters"].append(param)
        
        # Merge request body if present
        if chunk.get("requestBody"):
            if endpoints_by_path[path]["requestBody"]:
                if endpoints_by_path[path]["requestBody"] != chunk["requestBody"]:
                    logger.warning(f"Request body conflict for {path}")
            else:
                endpoints_by_path[path]["requestBody"] = chunk["requestBody"]
        
        # Merge responses
        for status_code, response in chunk.get("responses", {}).items():
            if status_code in endpoints_by_path[path]["responses"]:
                if endpoints_by_path[path]["responses"][status_code] != response:
                    logger.warning(f"Response conflict for status {status_code} in {path}")
            else:
                endpoints_by_path[path]["responses"][status_code] = response
    
    # Convert the nested structure to a flat list of endpoints
    final_endpoints = []
    for path, endpoint_info in endpoints_by_path.items():
        for method, method_info in endpoint_info["methods"].items():
            final_endpoint = {
                "endpoint": path,
                "method": method,
                "parameters": method_info["parameters"],
                "requestBody": method_info["requestBody"],
                "responses": method_info["responses"]
            }
            final_endpoints.append(final_endpoint)
    
    return final_endpoints

def generate_pytest_bdd_tests(swagger_url: str, model_name: str = "gpt-3.5-turbo") -> List[str]:
    """
    Generates pytest-bdd test files from a Swagger UI URL.
    
    Args:
        swagger_url: URL of the Swagger UI page
        model_name: Name of the LLM model to use
        
    Returns:
        List of paths to generated test files (both .feature and _steps.py files)
    """
    logger.info(f"Starting test generation from Swagger UI: {swagger_url}")
    generated_files = []
    
    try:
        # Step 1: Scrape Swagger UI
        logger.info("Scraping Swagger UI page...")
        operation_chunks = scrape_swagger_ui(swagger_url)
        if not operation_chunks:
            raise ValueError("No operation chunks found in Swagger UI")
        logger.info(f"Found {len(operation_chunks)} operation chunks")
        
        # Step 2: Extract endpoint information
        logger.info("Extracting endpoint information...")
        endpoint_chunks = []
        for i, chunk_html in enumerate(operation_chunks, 1):
            try:
                endpoint_info = extract_endpoint_info_via_llm(chunk_html, model_name=model_name)
                if endpoint_info:
                    endpoint_chunks.append(endpoint_info)
                    logger.debug(f"Successfully extracted info for chunk {i}")
                else:
                    logger.warning(f"No valid endpoint info found in chunk {i}")
            except json.JSONDecodeError as e:
                logger.error(f"Invalid JSON from LLM for chunk {i}: {str(e)}")
                continue
            except Exception as e:
                logger.error(f"Error processing chunk {i}: {str(e)}")
                continue
        
        if not endpoint_chunks:
            raise ValueError("No valid endpoint information extracted")
        logger.info(f"Successfully extracted info for {len(endpoint_chunks)} endpoints")
        
        # Step 3: Assemble endpoints
        logger.info("Assembling endpoint information...")
        all_endpoints = assemble_endpoints_from_chunks(endpoint_chunks)
        if not all_endpoints:
            raise ValueError("No valid endpoints after assembly")
        logger.info(f"Assembled {len(all_endpoints)} unique endpoints")
        
        # Step 4: Generate feature files and step definitions
        logger.info("Generating feature files and step definitions...")
        feature_files, step_files = generate_feature_files(all_endpoints, model_name=model_name)
        
        # Step 5: Generate CRUD scenarios
        logger.info("Generating CRUD scenarios...")
        crud_scenarios = generate_crud_e2e_scenarios(all_endpoints)
        crud_feature_files = []
        crud_step_files = []
        
        os.makedirs('features/crud', exist_ok=True)
        for feature_name, content in crud_scenarios.items():
            feature_path = f"features/crud/{feature_name}"
            with open(feature_path, 'w') as f:
                f.write(content)
            crud_feature_files.append(feature_path)
            
            # Generate step definitions for CRUD scenarios
            step_file = generate_step_definitions(feature_path, {'endpoint': '/crud', 'method': 'CRUD'})
            crud_step_files.append(step_file)
        
        # Combine all generated files
        generated_files = feature_files + step_files + crud_feature_files + crud_step_files
        
        # Step 6: Print summary
        logger.info("\nTest Generation Summary:")
        logger.info(f"Total Feature Files: {len(feature_files) + len(crud_feature_files)}")
        logger.info(f"Total Step Definition Files: {len(step_files) + len(crud_step_files)}")
        logger.info(f"Total Endpoints Processed: {len(all_endpoints)}")
        logger.info(f"Total Files Generated: {len(generated_files)}")
        
        return generated_files
        
    except Exception as e:
        logger.error(f"Error during test generation: {str(e)}")
        raise

if __name__ == "__main__":
    import argparse
    import mlflow

    parser = argparse.ArgumentParser(description="Generate pytest-bdd tests from a Swagger UI.")
    parser.add_argument("--swagger-url", type=str, help="URL of the Swagger UI", required=True)
    parser.add_argument("--model", type=str, default="gpt-3.5-turbo", help="Name of the LLM model to use")
    args = parser.parse_args()

    # Start MLflow Tracking
    mlflow.set_experiment("Swagger_Test_Generation")
    mlflow.start_run(run_name="Pytest_BDD_Generation")

    try:
        print("\n=== Generating pytest-bdd Tests from Swagger API ===")
        print("Swagger URL:", args.swagger_url)
        print("Model:", args.model)
        mlflow.log_param("swagger_url", args.swagger_url)
        mlflow.log_param("model_name", args.model)

        # Call the main generation function
        generated_files = generate_pytest_bdd_tests(args.swagger_url, model_name=args.model)

        print("\nGenerated Files:")
        for file in generated_files:
            print(f"- {file}")

        # Provide instructions for installing dependencies
        print("\n=== Setup Instructions ===")
        print("1. Install required packages:")
        print("   pip install requests-html pytest pytest-bdd python-dotenv openai mlflow")
        print("\n2. Ensure OPENAI_API_KEY is set as an environment variable")
        print("   export OPENAI_API_KEY=sk-xxxx")
        print("\n3. (Optional) Set up MLflow Tracking:")
        print("   export MLFLOW_TRACKING_URI=<your_tracking_uri>")
        print("\n4. To run tests and generate HTML report:")
        print("   pytest --html=reports/report.html --self-contained-html -v features/")
        print("\nSample command to generate tests:")
        print("   python main.py --swagger-url 'https://bookstore.toolsqa.com/swagger/'")
        print("\nTo see MLflow UI:")
        print("   mlflow ui")
        print("   # Then open http://127.0.0.1:5000 in a browser.")

        # End MLflow run
        mlflow.end_run()

    except Exception as e:
        print(f"Error: {str(e)}")
        mlflow.end_run(status="FAILED")
        sys.exit(1)